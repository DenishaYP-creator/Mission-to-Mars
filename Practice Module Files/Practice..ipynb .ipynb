{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e369236c",
   "metadata": {},
   "source": [
    "pip install splinter\n",
    "pip install webdriver_manager\n",
    "pip install bs4\n",
    "pip install html5lib\n",
    "pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611aa82",
   "metadata": {},
   "source": [
    "webdriver.Chrome('./home/user/drivers/chromedriver')\n",
    "ChromeDriverManager(path=custom_path).install()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789be937",
   "metadata": {},
   "source": [
    "# MODULE 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0215f7",
   "metadata": {},
   "source": [
    "1. Prep our automated browser by Creatingan instance of Splinter browser. \n",
    "2. Specify Chrome as our browser. \n",
    "3. **executable_path is unpacking the dictionary we've stored the path in – think of it as unpacking a suitcase. \n",
    "4. headless=False means that all of the browser's actions will be displayed in a Chrome window so we can see them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201144dc",
   "metadata": {},
   "source": [
    "# 10.3.2 Practice with Splinter and BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5985ce",
   "metadata": {},
   "source": [
    "### PREP SPLINTER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6a4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the tools Splinter and BeautifulSoup. using an alias, \"soup,\"\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02259628",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChromeDriverManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-65461b746a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#set the executable path and initialize a browser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Set up Splinter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexecutable_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'executable_path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChromeDriverManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBrowser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chrome'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChromeDriverManager' is not defined"
     ]
    }
   ],
   "source": [
    "#set the executable path and initialize a browser.\n",
    "#Set up Splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13a515",
   "metadata": {},
   "source": [
    "### Scrape the Top 10 Tags\n",
    "#### http://quotes.toscrape.com/\n",
    "Our goal for this practice is to scrape the \"Top Ten tags\" text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203d10f",
   "metadata": {},
   "source": [
    "### Scrape the Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code tells Splinter which site we want to visit by assigning the link to a URL.\n",
    "# Visit the Quotes to Scrape site\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE BeautifulSoup to parse the HTML.\n",
    "# Parse the HTML\n",
    "html = browser.html\n",
    "html_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d73cc",
   "metadata": {},
   "source": [
    "#### In our code, we're using ‘html.parser’ to parse the information, but there are other options available as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the title and extract it.\n",
    "# Scrape the Title\n",
    "title = html_soup.find('h2').text\n",
    "title\n",
    "#We used our html_soup object we created earlier and chained find() to it to search for the <h2 /> tag.\n",
    "#We've also extracted only the text within the HTML tags by adding .text to the end of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f444cc",
   "metadata": {},
   "source": [
    "### Scrape All of the Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the top ten tags\n",
    "tag_box = html_soup.find('div', class_='tags-box')\n",
    "# tag_box\n",
    "tags = tag_box.find_all('a', class_='tag')\n",
    "\n",
    "for tag in tags:\n",
    "    word = tag.text\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7297854",
   "metadata": {},
   "source": [
    "\n",
    "1. The first line, tag_box = html_soup.find('div', class_='tags-box'), creates a new variable tag_box, which will be used to store the results of a search. In this case, we're looking for <div /> elements with a class of tags-box, and we're searching for it in the HTML we parsed earlier and stored in the html_soup variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf64811",
   "metadata": {},
   "source": [
    "2. The second line, tags = tag_box.find_all('a', class_='tag'), is similar to the first but with a few tweaks to make the search more specific. The new \"tags\" variable will hold the results of a find_all, but this time we're searching through the parsed results stored in our tag_box variable to find <a /> elements with a tag class.\n",
    "- We used find_all this time because we want to capture all results, instead of a single or specific one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36973a4e",
   "metadata": {},
   "source": [
    "3. Next, we've added a for loop. This for loop cycles through each tag in the tags variable, strips the HTML code out of it, and then prints only the text of each tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f801f3",
   "metadata": {},
   "source": [
    "### Scrape Across Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d2420",
   "metadata": {},
   "source": [
    "Our next section of code will scrape the quotes on the first page, click the \"Next\" button, then scrape more quotes and so on (five pages worth of quotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48bb9e",
   "metadata": {},
   "source": [
    "THe above Lines They assign an actual URL to the variable named \"url\" and then tell Splinter to visit that webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ab94d",
   "metadata": {},
   "source": [
    "#### create a for loop to collect each quote, \"click\" the next button, then collect the next set of quotes. We'll use range(1, 6) in our for loop to visit the first five pages of the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee74349",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1, 6):\n",
    "   html = browser.html\n",
    "   quote_soup = soup (html, 'html.parser')\n",
    "   quotes = quote_soup.find_all('span', class_='text')\n",
    "   for quote in quotes:\n",
    "      print('page:', x, '----------')\n",
    "      print(quote.text)\n",
    "   browser.links.find_by_partial_text('Next')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7777e",
   "metadata": {},
   "source": [
    "# SKILL DRILL\n",
    "Stretch your scraping skills by visiting Books to Scrape (Links to an external site.) and scraping the book URL list on the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc61cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
